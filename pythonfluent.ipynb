{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7dd8bad",
   "metadata": {},
   "source": [
    "# Python Fluent Interface / Method Chaining\n",
    "Ich wollte einmal hier festhalten wie man sogenannte \"Fluent Interfaces\" in Python schreibt, mit einem kleinen technischen DeepDive zu Python am Ende.\n",
    "\n",
    "Hintergrund war, in Fabric, kann man nicht ohne weiteres Tabellen bearbeiten, ärgerlich vor allem wenn man \"Configurations-Tabellen\" hat die man oft anpassen oder erweitern möchte. Ein kleines \"Workbench\"-Python Notebook ist da quasi die schnellste Methode.\n",
    "\n",
    "Nach dem ich zum fünften mal so etwas getippt habe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a42bfbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when, col\n",
    "df = spark.sql(\"SELECT * FROM Foo.dbo.data_processing_information\")\n",
    "updated_df = df.withColumn( \"primaryKeyColumns\", when(col(\"Fileprefix\") == \"SL_GP_Basis\", \"Foo;bar\") .otherwise(col(\"primaryKeyColumns\")))\n",
    "display(updated_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c35ea02",
   "metadata": {},
   "source": [
    "Habe ich gedacht es wird Zeit für eine Helfer-Funktion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a440ab38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mocks\n",
    "from typing import Any\n",
    "\n",
    "class Mock:\n",
    "    def __init__(self, name: str, return_extra: Any = None):\n",
    "        \"\"\"Initializes the mock object.\"\"\"\n",
    "        self.name = name\n",
    "        self.return_extra = return_extra\n",
    "    def __call__(self, *args, **kwargs) -> Any:\n",
    "        \"\"\"This makes instances of the class callable, like a function.\"\"\"\n",
    "        print(f\"{self.name} <- {args}, {kwargs}\")\n",
    "        return self.return_extra\n",
    "    def __repr__(self) -> str:\n",
    "        \"\"\"Provides the custom, developer-friendly representation.\"\"\"\n",
    "        # This is what you see when you just type the object's name\n",
    "        # in a notebook cell or use print().\n",
    "        return f\"<{self.name}-mock>\"\n",
    "    \n",
    "class MSpark:    \n",
    "    class MDf:\n",
    "        withColumn = Mock(\"withColumn\")\n",
    "    sql = Mock(\"spark.sql\", MDf())\n",
    "    table = Mock(\"spark.table\", MDf())\n",
    "spark = MSpark()\n",
    "class MOther:\n",
    "    otherwise = Mock(\"otherwise\")\n",
    "when = Mock(\"when\", MOther())\n",
    "col= Mock(\"col\")\n",
    "lit = Mock(\"lit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4be10942",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark.sql <- ('SELECT * FROM Foo.dbo.data_processing_information',), {}\n",
      "col <- ('Fileprefix',), {}\n",
      "when <- (False, 'Foo;bar'), {}\n",
      "col <- ('primaryKeyColumns',), {}\n",
      "otherwise <- (None,), {}\n",
      "withColumn <- ('primaryKeyColumns', None), {}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def updateRowField(table, column, whereCol, whereVal, value):\n",
    "    df = spark.sql(f\"SELECT * FROM {table}\")\n",
    "    updated_df = df.withColumn( column, when(col(whereCol) == whereVal, value) .otherwise(col(column)))\n",
    "    display(updated_df)\n",
    "    \n",
    "updateRowField(table=\"Foo.dbo.data_processing_information\", column=\"primaryKeyColumns\",whereCol=\"Fileprefix\", whereVal=\"SL_GP_Basis\",value=\"Foo;bar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192acd36",
   "metadata": {},
   "source": [
    "Was aber immernoch nicht besonders Userfreundlich ist.\n",
    "\n",
    "# Fluent Interfaces\n",
    "Ich hätte gerne, wie es Assertion-frameworks oder DataFrames machen, so etwas geschrieben: `udpated_df = doSet(\"Foo.dbo.data_processing_information\").rowWhereColumn('Fileprefix').equals('SL_GP_Basis').changeColumn('primaryKeyColumns').to(\"Foo;bar\")`\n",
    "\n",
    "Dafür bietet sich eine Helper-Klasse an."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "52a26402",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparkTableUpdater:\n",
    "    \"\"\"\n",
    "    A fluent interface helper to perform a conditional update on a Spark DataFrame.\n",
    "    \"\"\"\n",
    "    def __init__(self, table_name: str):\n",
    "        # Initial state\n",
    "        self.spark = spark\n",
    "        self._table_name = table_name\n",
    "        self._df = None\n",
    "        \n",
    "        # State to be built by the chain\n",
    "        self._filter_column = None\n",
    "        self._filter_value = None\n",
    "        self._update_column = None\n",
    "\n",
    "    def rowWhereColumn(self, column_name: str):\n",
    "        \"\"\"Specifies the column for the WHERE clause.\"\"\"\n",
    "        print(f\"STATE: Setting filter column to '{column_name}'\")\n",
    "        self._filter_column = column_name\n",
    "        return self # <-- The magic! Return the object itself.\n",
    "\n",
    "    def equals(self, value: any):\n",
    "        \"\"\"Specifies the value to check for equality in the WHERE clause.\"\"\"\n",
    "        print(f\"STATE: Setting filter value to '{value}'\")\n",
    "        self._filter_value = value\n",
    "        return self # <-- Return the object to allow further chaining.\n",
    "\n",
    "    def changeColumn(self, column_name: str):\n",
    "        \"\"\"Specifies the column to be updated.\"\"\"\n",
    "        print(f\"STATE: Setting column to update to '{column_name}'\")\n",
    "        self._update_column = column_name\n",
    "        return self # <-- Again, return self.\n",
    "\n",
    "    def to(self, new_value: any):\n",
    "        \"\"\"\n",
    "        TERMINATING METHOD: Executes the update and returns the final DataFrame.\n",
    "        This is the end of the chain.\n",
    "        \"\"\"\n",
    "        print(\"ACTION: Executing the update...\")\n",
    "        # --- Validation ---\n",
    "        if not all([self._filter_column, self._update_column]):\n",
    "            raise ValueError(\"Incomplete chain. You must call rowWhereColumn(), equals(), and changeColumn() before .to()\")\n",
    "        \n",
    "        # --- Execution ---\n",
    "        # 1. Load the initial DataFrame\n",
    "        df = self.spark.table(self._table_name)\n",
    "\n",
    "        # 2. Apply the logic using standard PySpark functions\n",
    "        updated_df = df.withColumn(\n",
    "            self._update_column,\n",
    "            when(col(self._filter_column) == self._filter_value, lit(new_value))\n",
    "            .otherwise(col(self._update_column))\n",
    "        )\n",
    "        \n",
    "        # 3. This method returns the final result, not 'self'\n",
    "        return updated_df\n",
    "\n",
    "# Optional: A factory function to make the start of the chain look cleaner\n",
    "def doSet(table_name: str):\n",
    "    \"\"\"Factory function to initialize the SparkTableUpdater.\"\"\"\n",
    "    return SparkTableUpdater(table_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ce7310",
   "metadata": {},
   "source": [
    "Dann funktioniert:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14baf726",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STATE: Setting filter column to 'Fileprefix'\n",
      "STATE: Setting filter value to 'SL_GP_Basis'\n",
      "STATE: Setting column to update to 'primaryKeyColumns'\n",
      "ACTION: Executing the update...\n",
      "spark.table <- ('Foo.dbo.data_processing_information',), {}\n",
      "col <- ('Fileprefix',), {}\n",
      "lit <- ('Foo;bar',), {}\n",
      "when <- (False, None), {}\n",
      "col <- ('primaryKeyColumns',), {}\n",
      "otherwise <- (None,), {}\n",
      "withColumn <- ('primaryKeyColumns', None), {}\n",
      "STATE: Setting filter value to 'SL_GP_Basis'\n",
      "STATE: Setting filter column to 'Fileprefix'\n",
      "STATE: Setting column to update to 'primaryKeyColumns'\n",
      "ACTION: Executing the update...\n",
      "spark.table <- ('Foo.dbo.data_processing_information',), {}\n",
      "col <- ('Fileprefix',), {}\n",
      "lit <- ('Foo;bar',), {}\n",
      "when <- (False, None), {}\n",
      "col <- ('primaryKeyColumns',), {}\n",
      "otherwise <- (None,), {}\n",
      "withColumn <- ('primaryKeyColumns', None), {}\n",
      "STATE: Setting filter value to 'SL_GP_Basis'\n",
      "ACTION: Executing the update...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Incomplete chain. You must call rowWhereColumn(), equals(), and changeColumn() before .to()",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m doSet(\u001b[33m\"\u001b[39m\u001b[33mFoo.dbo.data_processing_information\u001b[39m\u001b[33m\"\u001b[39m).equals(\u001b[33m'\u001b[39m\u001b[33mSL_GP_Basis\u001b[39m\u001b[33m'\u001b[39m).rowWhereColumn(\u001b[33m'\u001b[39m\u001b[33mFileprefix\u001b[39m\u001b[33m'\u001b[39m).changeColumn(\u001b[33m'\u001b[39m\u001b[33mprimaryKeyColumns\u001b[39m\u001b[33m'\u001b[39m).to(\u001b[33m\"\u001b[39m\u001b[33mFoo;bar\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Fehler output\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[43mdoSet\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mFoo.dbo.data_processing_information\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mequals\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mSL_GP_Basis\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mFoo;bar\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 42\u001b[39m, in \u001b[36mSparkTableUpdater.to\u001b[39m\u001b[34m(self, new_value)\u001b[39m\n\u001b[32m     40\u001b[39m \u001b[38;5;66;03m# --- Validation ---\u001b[39;00m\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mall\u001b[39m([\u001b[38;5;28mself\u001b[39m._filter_column, \u001b[38;5;28mself\u001b[39m._update_column]):\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mIncomplete chain. You must call rowWhereColumn(), equals(), and changeColumn() before .to()\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     44\u001b[39m \u001b[38;5;66;03m# --- Execution ---\u001b[39;00m\n\u001b[32m     45\u001b[39m \u001b[38;5;66;03m# 1. Load the initial DataFrame\u001b[39;00m\n\u001b[32m     46\u001b[39m df = \u001b[38;5;28mself\u001b[39m.spark.table(\u001b[38;5;28mself\u001b[39m._table_name)\n",
      "\u001b[31mValueError\u001b[39m: Incomplete chain. You must call rowWhereColumn(), equals(), and changeColumn() before .to()"
     ]
    }
   ],
   "source": [
    "doSet(\"Foo.dbo.data_processing_information\").rowWhereColumn('Fileprefix').equals('SL_GP_Basis').changeColumn('primaryKeyColumns').to(\"Foo;bar\")\n",
    "doSet(\"Foo.dbo.data_processing_information\").equals('SL_GP_Basis').rowWhereColumn('Fileprefix').changeColumn('primaryKeyColumns').to(\"Foo;bar\")\n",
    "# Fehler output\n",
    "doSet(\"Foo.dbo.data_processing_information\").equals('SL_GP_Basis').to(\"Foo;bar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b466d776",
   "metadata": {},
   "source": [
    "Der Nachteil ist das `doSet(\"Foo.dbo.data_processing_information\").equals('SL_GP_Basis').rowWhereColumn('Fileprefix').changeColumn('primaryKeyColumns').to(\"Foo;bar\")` genauso funktioniert aber nicht mehr lesbar ist. Um die Reihenfolgen zu erzwingen könnte man interne Klassen verwenden.\n",
    "\n",
    "## State-Machine Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc68914",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparkTableUpdater2:\n",
    "    \"\"\"\n",
    "    A fluent, state-safe interface to perform a conditional update on a Spark DataFrame.\n",
    "    Uses nested classes to enforce method order and encapsulate state.\n",
    "    \"\"\"\n",
    "\n",
    "    # --- Step 1: The Entry Point (Outer Class) ---\n",
    "    def __init__(self, table_name: str):\n",
    "        self._spark = spark\n",
    "        self._table_name = table_name\n",
    "        \n",
    "        # These will be set by the inner classes\n",
    "        self._filter_column = None\n",
    "        self._operator = None\n",
    "        self._filter_value = None\n",
    "        self._update_column = None\n",
    "\n",
    "    def rowWhereColumn(self, column_name: str):\n",
    "        \"\"\"Specifies the column for the WHERE clause.\"\"\"\n",
    "        print(f\"STATE: Setting filter column to '{column_name}'\")\n",
    "        self._filter_column = column_name\n",
    "        return self._ConditionBuilder(self) # Pass a reference to the outer instance\n",
    "\n",
    "    # --- Step 2: Nested Class for Building the Condition ---\n",
    "    class _ConditionBuilder:\n",
    "        def __init__(self, updater_instance: \"SparkTableUpdater2\"):\n",
    "            # Store a reference to the outer SparkTableUpdater instance\n",
    "            self.outer = updater_instance\n",
    "\n",
    "        def equals(self, value: any):\n",
    "            \"\"\"Specifies the value to check for equality.\"\"\"\n",
    "            print(f\"STATE: Setting filter operator to '==' and value to '{value}'\")\n",
    "            self.outer._operator = \"==\"\n",
    "            self.outer._filter_value = value\n",
    "            return self.outer._TargetColumnSelector(self.outer)\n",
    "\n",
    "        def isGreaterThan(self, value: any):\n",
    "            \"\"\"Specifies the value to check for greater than.\"\"\"\n",
    "            print(f\"STATE: Setting filter operator to '>' and value to '{value}'\")\n",
    "            self.outer._operator = \">\"\n",
    "            self.outer._filter_value = value\n",
    "            return self.outer._TargetColumnSelector(self.outer)\n",
    "\n",
    "    # --- Step 3: Nested Class for Selecting the Target Column ---\n",
    "    class _TargetColumnSelector:\n",
    "        def __init__(self, updater_instance: \"SparkTableUpdater2\"):\n",
    "            self.outer = updater_instance\n",
    "        \n",
    "        def changeColumn(self, column_name: str):\n",
    "            \"\"\"Specifies the column to be updated.\"\"\"\n",
    "            print(f\"STATE: Setting column to update to '{column_name}'\")\n",
    "            self.outer._update_column = column_name\n",
    "            return self.outer._UpdateExecutor(self.outer)\n",
    "\n",
    "    # --- Step 4: Nested Class for the Final Execution ---\n",
    "    class _UpdateExecutor:\n",
    "        def __init__(self, updater_instance: \"SparkTableUpdater2\"):\n",
    "            self.outer = updater_instance\n",
    "\n",
    "        def to(self, new_value: any):\n",
    "            \"\"\"TERMINATING METHOD: Executes the update and returns the final DataFrame.\"\"\"\n",
    "            print(\"ACTION: Executing the update...\")\n",
    "            \n",
    "            # Access attributes directly from the outer instance\n",
    "            df = self.outer._spark.table(self.outer._table_name)\n",
    "            \n",
    "            condition = None\n",
    "            if self.outer._operator == \"==\":\n",
    "                condition = (col(self.outer._filter_column) == self.outer._filter_value)\n",
    "            elif self.outer._operator == \">\":\n",
    "                 condition = (col(self.outer._filter_column) > self.outer._filter_value)\n",
    "            else:\n",
    "                raise NotImplementedError(f\"Operator '{self.outer._operator}' not supported.\")\n",
    "\n",
    "            updated_df = df.withColumn(\n",
    "                self.outer._update_column,\n",
    "                when(condition, lit(new_value))\n",
    "                .otherwise(col(self.outer._update_column))\n",
    "            )\n",
    "            return updated_df\n",
    "        \n",
    "def doSet2(table_name: str):\n",
    "    \"\"\"Factory function to initialize the SparkTableUpdater.\"\"\"\n",
    "    return SparkTableUpdater2(table_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f885fd96",
   "metadata": {},
   "source": [
    "Jetzt kann der Entwickler die Methoden nur noch in der vorgegebenen Reihenfolge verwenden. Außerdem meldet die IDE die verfügbaren Methoden via auto-complete und bemängelt falsches Verwenden direkt im Editor.\n",
    "\n",
    "### Side-Note\n",
    "\n",
    "In den Unterklassen von `SparkTableUpdater2` zum Zeitpunkt wenn die Methoden-Signaturen initialisiert werden, exisitiert der Typ \"SparkTableUpdater2\" noch nicht. Deswegen muss der Type hier mit Anführungszeichen `__init__(self, updater_instance: \"SparkTableUpdater2\")` angegeben werden. Dies meldet dem Language-Server das dieser Type erst \"nachträglich\" verfügbar ist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81b9861",
   "metadata": {},
   "outputs": [],
   "source": [
    "doSet2(\"Foo.dbo.data_processing_information\").rowWhereColumn('Fileprefix').equals('SL_GP_Basis').changeColumn('primaryKeyColumns').to(\"Foo;bar\")\n",
    "# Fehler output\n",
    "# Cannot access attribute \"equals\" for class \"SparkTableUpdater2\"\n",
    "#  Attribute \"equals\" is unknownPylancereportAttributeAccessIssue\n",
    "# doSet2(\"Foo.dbo.data_processing_information\").~~equals('SL_GP_Basis')~~.rowWhereColumn('Fileprefix').changeColumn('primaryKeyColumns').to(\"Foo;bar\")\n",
    "f = open(\"\")\n",
    "f.__class__.__base__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0f637d",
   "metadata": {},
   "source": [
    "# Zuviel Boiler-Plate? Python unter der Haube.\n",
    "Das ist aber alles recht viel Schreibaufwand für \"nur\" eine Hilfs-Funktion. Aber hier kann man eine Eigenschaft von Python als interpretierte \"Duck-Typed\" Sprache zu nutzen machen.\n",
    "\n",
    "### The \"Magic\" Method: __getattr__\n",
    "In Python, wenn man versucht auf ein Attribute `my_thing.something` zuzugreifen, schaut Python an mehere stellen. Als erstes im `my_thing.__dict__` Dictionary der Instanz, dann ihrer Klasse `my_thing.__class__`, Super-Klasse `my_thing.__class__.__base__`. Wenn all diese Orte nichts liefern, dann ruft Python als letzter Versuch `my_thing.__getattr__(self, name)` auf, wenn es auf der Instanz existiert.\n",
    "\n",
    "Diese Methode kann man \"hijacken\" um die Fluent API in einer einzigen Klasse zu implementieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b4bfc774",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "class DynamicUpdater:\n",
    "    # Define the valid methods for each state of our fluent API\n",
    "    _VALID_METHODS = {\n",
    "        'initial': {'rowWhereColumn'},\n",
    "        'condition_set': {'equals', 'isGreaterThan'},\n",
    "        'operator_set': {'changeColumn'},\n",
    "        'target_set': {'to'}\n",
    "    }\n",
    "\n",
    "    def __init__(self, table_name: str):\n",
    "        self._spark = spark\n",
    "        self._table_name = table_name\n",
    "        \n",
    "        # The current state of the builder\n",
    "        self._state = 'initial'\n",
    "        \n",
    "        # Placeholders for the query parts\n",
    "        self._query_parts = {}\n",
    "\n",
    "    def __getattr__(self, name):\n",
    "        # This magic method is called ONLY when an attribute is not found\n",
    "        \n",
    "        # 1. Is the called method valid for the current state?\n",
    "        if name in self._VALID_METHODS.get(self._state, set()):\n",
    "            # 2. Yes. Return a handler function that will execute the logic.\n",
    "            #    We use `partial` to pre-fill the method name for our dispatcher.\n",
    "            return partial(self._dispatcher, name)\n",
    "        \n",
    "        # 3. No. The method call is invalid for the current state.\n",
    "        raise AttributeError(\n",
    "            f\"'{type(self).__name__}' object has no attribute '{name}' in state '{self._state}'. \"\n",
    "            f\"Valid methods: {self._VALID_METHODS.get(self._state, 'None')}\"\n",
    "        )\n",
    "\n",
    "    def _dispatcher(self, method_name, *args, **kwargs):\n",
    "        \"\"\"A central place to handle the logic for each valid method.\"\"\"\n",
    "        \n",
    "        # --- Logic for 'rowWhereColumn' ---\n",
    "        if method_name == 'rowWhereColumn':\n",
    "            self._query_parts['filter_col'] = args[0]\n",
    "            self._state = 'condition_set' # Transition to the next state\n",
    "\n",
    "        # --- Logic for operators ---\n",
    "        elif method_name in ('equals', 'isGreaterThan'):\n",
    "            operator_map = {'equals': '==', 'isGreaterThan': '>'}\n",
    "            self._query_parts['operator'] = operator_map[method_name]\n",
    "            self._query_parts['filter_val'] = args[0]\n",
    "            self._state = 'operator_set' # Transition to the next state\n",
    "        \n",
    "        # --- Logic for 'changeColumn' ---\n",
    "        elif method_name == 'changeColumn':\n",
    "            self._query_parts['update_col'] = args[0]\n",
    "            self._state = 'target_set' # Transition to the next state\n",
    "\n",
    "        # --- Terminating logic for 'to' ---\n",
    "        elif method_name == 'to':\n",
    "            # This is the final method, it does not return self\n",
    "            return self._execute_update(args[0])\n",
    "\n",
    "        # Return self to allow method chaining\n",
    "        return self\n",
    "\n",
    "    def _execute_update(self, new_value):\n",
    "        \"\"\"The final execution logic, pulled from our previous example.\"\"\"\n",
    "        qp = self._query_parts\n",
    "        df = self._spark.table(self._table_name)\n",
    "\n",
    "        condition = None\n",
    "        if qp['operator'] == '==':\n",
    "            condition = (col(qp['filter_col']) == qp['filter_val'])\n",
    "        elif qp['operator'] == '>':\n",
    "            condition = (col(qp['filter_col']) > qp['filter_val'])\n",
    "        \n",
    "        return df.withColumn(\n",
    "            qp['update_col'],\n",
    "            when(condition, lit(new_value)).otherwise(col(qp['update_col']))\n",
    "        )\n",
    "\n",
    "# Factory function remains the same\n",
    "def doDynamicSet(table_name: str) -> DynamicUpdater:\n",
    "    return DynamicUpdater(table_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9d92cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark.table <- ('Foo.dbo.data_processing_information',), {}\n",
      "col <- ('Fileprefix',), {}\n",
      "lit <- ('Foo;bar',), {}\n",
      "when <- (False, None), {}\n",
      "col <- ('primaryKeyColumns',), {}\n",
      "otherwise <- (None,), {}\n",
      "withColumn <- ('primaryKeyColumns', None), {}\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'DynamicUpdater' object has no attribute 'foo' in state 'initial'. Valid methods: {'rowWhereColumn'}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m doDynamicSet(\u001b[33m\"\u001b[39m\u001b[33mFoo.dbo.data_processing_information\u001b[39m\u001b[33m\"\u001b[39m).rowWhereColumn(\u001b[33m'\u001b[39m\u001b[33mFileprefix\u001b[39m\u001b[33m'\u001b[39m).equals(\u001b[33m'\u001b[39m\u001b[33mSL_GP_Basis\u001b[39m\u001b[33m'\u001b[39m).changeColumn(\u001b[33m'\u001b[39m\u001b[33mprimaryKeyColumns\u001b[39m\u001b[33m'\u001b[39m).to(\u001b[33m\"\u001b[39m\u001b[33mFoo;bar\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mdoDynamicSet\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mFehler\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfoo\u001b[49m(\u001b[33m'\u001b[39m\u001b[33mstuff\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 32\u001b[39m, in \u001b[36mDynamicUpdater.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m     29\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m partial(\u001b[38;5;28mself\u001b[39m._dispatcher, name)\n\u001b[32m     31\u001b[39m \u001b[38;5;66;03m# 3. No. The method call is invalid for the current state.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[32m     33\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m object has no attribute \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m in state \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m._state\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     34\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mValid methods: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m._VALID_METHODS.get(\u001b[38;5;28mself\u001b[39m._state,\u001b[38;5;250m \u001b[39m\u001b[33m'\u001b[39m\u001b[33mNone\u001b[39m\u001b[33m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     35\u001b[39m )\n",
      "\u001b[31mAttributeError\u001b[39m: 'DynamicUpdater' object has no attribute 'foo' in state 'initial'. Valid methods: {'rowWhereColumn'}"
     ]
    }
   ],
   "source": [
    "doDynamicSet(\"Foo.dbo.data_processing_information\").rowWhereColumn('Fileprefix').equals('SL_GP_Basis').changeColumn('primaryKeyColumns').to(\"Foo;bar\")\n",
    "doDynamicSet('Fehler').foo('stuff')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80848c1",
   "metadata": {},
   "source": [
    "Der Nachteil ist, dass die verfügbaren Methoden auf einen dynamischen Laufzeit-Zustand beruhen. Daher verliert man die Hilfe des Linters/IDE. Also in diesem Fall eher ein Akademisches Beispiel als eine empfohlene Art und Weiße das Kontext-Problem zu lösen.\n",
    "\n",
    "### Mehr Magie: Class Factories\n",
    "Instanzen werden von Python über die `__new__` Methode an der Klasse selbst erzeugt. Diese kann man überschreiben wenn man die Instanzierung der Objekte manipulieren möchte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fa1faed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Type, TypeVar, cast\n",
    "\n",
    "\n",
    "A = TypeVar(\"A\")\n",
    "\n",
    "class Agent:\n",
    "    def run(self, text):\n",
    "        print(text)\n",
    "\n",
    "class APrinterAgent(Agent):\n",
    "    def run(self, text):\n",
    "        super().run(f\"A{text}A\")\n",
    "\n",
    "class BPrinterAgent(Agent):\n",
    "    def run(self, text):\n",
    "        super().run(f\"B{text}B\")\n",
    "        \n",
    "class MultiShotAgent[A: Agent]:\n",
    "    def __new__(\n",
    "        cls,\n",
    "        agent_class: Type[A], \n",
    "        repeat: int = 3,\n",
    "        *args,\n",
    "        **kwargs\n",
    "    ) -> A: # The return type is an instance of the provided agent class\n",
    "        \n",
    "        def multi_run(self, text):\n",
    "            text = \"\".join([text]* self.repeat)\n",
    "            # This super() will find the 'run' method on the parent (e.g., BPrinterAgent)\n",
    "            super(dynamic_multi_agent, self).run(text)\n",
    "\n",
    "        dynamic_multi_agent = type(\n",
    "            f\"MultiShot{agent_class.__name__}\",\n",
    "            (agent_class,),  # Inherit from the provided agent class\n",
    "            {\n",
    "                \"run\": multi_run, # Use the new method we just defined\n",
    "                \"repeat\": repeat  # Add repeat as a class attribute\n",
    "            }\n",
    "        )\n",
    "\n",
    "        #    We cast it to 'A' which is the TypeVar for the agent_class\n",
    "        instance = cast(A, dynamic_multi_agent(*args, **kwargs))\n",
    "        return instance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7c5a3958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AfooA\n",
      "BfooB\n",
      "BfoofoofoofoofooB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "APrinterAgent().run(\"foo\")\n",
    "bp = BPrinterAgent()\n",
    "bp.run(\"foo\")\n",
    "mbp: BPrinterAgent\n",
    "mbp = MultiShotAgent(BPrinterAgent,5)\n",
    "mbp.run(\"foo\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea320fc2",
   "metadata": {},
   "source": [
    "Diese Fluent Api könnte man also auch so definieren:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b0d2a316",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: The Blueprint. This data structure IS our API definition.\n",
    "API_BLUEPRINT = {\n",
    "    'initial': {\n",
    "        'methods': {\n",
    "            # method_name: (state_key_to_set, next_state_name)\n",
    "            'rowWhereColumn': ('filter_col', 'condition_builder'),\n",
    "        }\n",
    "    },\n",
    "    'condition_builder': {\n",
    "        'methods': {\n",
    "            'equals': ('filter_val', 'target_selector'),\n",
    "            'isGreaterThan': ('filter_val', 'target_selector'),\n",
    "        },\n",
    "        # We can add extra logic, like mapping method names to operators\n",
    "        'operator_map': {'equals': '==', 'isGreaterThan': '>'}\n",
    "    },\n",
    "    'target_selector': {\n",
    "        'methods': {\n",
    "            'changeColumn': ('update_col', 'executor'),\n",
    "        }\n",
    "    },\n",
    "    'executor': {\n",
    "        'methods': {\n",
    "            # The terminator is special, it has no next state.\n",
    "            'to': (None, None),\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "def _execute_final_update(query_parts, spark_session, table_name):\n",
    "    \"\"\"The final action. This is normal, non-meta code.\"\"\"\n",
    "    df = spark_session.table(table_name)\n",
    "    qp = query_parts\n",
    "    \n",
    "    condition = None\n",
    "    if qp['operator'] == '==':\n",
    "        condition = (col(qp['filter_col']) == qp['filter_val'])\n",
    "    elif qp['operator'] == '>':\n",
    "        condition = (col(qp['filter_col']) > qp['filter_val'])\n",
    "\n",
    "    return df.withColumn(\n",
    "        qp['update_col'],\n",
    "        when(condition, lit(qp['new_value'])).otherwise(col(qp['update_col']))\n",
    "    )\n",
    "\n",
    "\n",
    "class FluentAPI:\n",
    "    def __new__(cls, state_name: str):\n",
    "        \"\"\"Dynamically creates a class type for a given state in the API chain.\"\"\"\n",
    "        \n",
    "        class_name = f\"DynamicState_{state_name.title().replace('_','')}\"\n",
    "        state_blueprint = API_BLUEPRINT[state_name]\n",
    "        \n",
    "        # This is the handler that will become a method on our new class.\n",
    "        def _make_method_handler(method_name, key_to_set, next_state):\n",
    "            # This is a closure. It captures the variables from its defining scope.\n",
    "            def handler(self, value):\n",
    "                # 1. Update the shared query parts dictionary\n",
    "                self.query_parts[key_to_set] = value\n",
    "                \n",
    "                # Special handling for operators\n",
    "                if 'operator_map' in state_blueprint:\n",
    "                    self.query_parts['operator'] = state_blueprint['operator_map'][method_name]\n",
    "\n",
    "                # 2. Create the class for the NEXT state in the chain\n",
    "                NextStateClass = cls(next_state)\n",
    "                \n",
    "                # 3. Return an INSTANCE of the new class, passing the state along\n",
    "                return NextStateClass(self.query_parts, self.spark, self.table_name)\n",
    "            return handler\n",
    "\n",
    "        # The handler for the terminating 'to' method is different\n",
    "        def _terminating_handler(self, value):\n",
    "            self.query_parts['new_value'] = value\n",
    "            # It doesn't create a new class, it executes the final logic.\n",
    "            return _execute_final_update(self.query_parts, self.spark, self.table_name)\n",
    "\n",
    "        # --- Assemble the new class ---\n",
    "        class_attributes = {}\n",
    "        \n",
    "        # The __init__ for every dynamic class will be the same.\n",
    "        def __init__(self, query_parts, spark, table_name):\n",
    "            self.query_parts = query_parts\n",
    "            self.spark = spark\n",
    "            self.table_name = table_name\n",
    "        class_attributes['__init__'] = __init__\n",
    "\n",
    "        # Dynamically create and attach the methods for this state\n",
    "        for method_name, (key_to_set, next_state) in state_blueprint['methods'].items():\n",
    "            if method_name == 'to':\n",
    "                class_attributes[method_name] = _terminating_handler\n",
    "            else:\n",
    "                class_attributes[method_name] = _make_method_handler(method_name, key_to_set, next_state)\n",
    "                \n",
    "        # The magic: type(ClassName, (Bases,), {Attributes}) creates a new class object\n",
    "        return type(class_name, (object,), class_attributes)\n",
    "\n",
    "\n",
    "# Step 3: The User-Facing Factory Function\n",
    "def doSetWithFactory( table_name: str):\n",
    "    \"\"\"The entry point that kicks off the dynamic class generation chain.\"\"\"\n",
    "    # Create the initial class for the 'initial' state\n",
    "    InitialStateClass = FluentAPI('initial')\n",
    "    \n",
    "    # Return an instance of it, starting with an empty state dictionary\n",
    "    return InitialStateClass(query_parts={}, spark=spark, table_name=table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f6e6b23e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark.table <- ('Foo.dbo.data_processing_information',), {}\n",
      "col <- ('Fileprefix',), {}\n",
      "lit <- ('Foo;bar',), {}\n",
      "when <- (False, None), {}\n",
      "col <- ('primaryKeyColumns',), {}\n",
      "otherwise <- (None,), {}\n",
      "withColumn <- ('primaryKeyColumns', None), {}\n"
     ]
    }
   ],
   "source": [
    "doSetWithFactory(\"Foo.dbo.data_processing_information\").rowWhereColumn('Fileprefix').equals('SL_GP_Basis').changeColumn('primaryKeyColumns').to(\"Foo;bar\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
